{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64c3214-c2ab-463a-840c-a14c8a70ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing node features\n",
      "Pre-processing node features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.pyenv/versions/3.8.5/lib/python3.8/pickle.py:1581: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  return getattr(sys.modules[module], name)\n",
      "/home/daniel/git/mot/.venv/lib/python3.8/site-packages/spektral/utils/io.py:25: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  return pickle.load(f, encoding=\"latin1\")\n",
      "/home/daniel/git/mot/.venv/lib/python3.8/site-packages/spektral/datasets/citation.py:108: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  a = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # CSR\n",
      "/home/daniel/git/mot/.venv/lib/python3.8/site-packages/networkx/linalg/graphmatrix.py:173: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `to_scipy_sparse_array` instead.\n",
      "  return nx.to_scipy_sparse_matrix(G, nodelist=nodelist, dtype=dtype, weight=weight)\n",
      "/home/daniel/git/mot/.venv/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "/home/daniel/git/mot/.venv/lib/python3.8/site-packages/spektral/datasets/citation.py:189: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "from spektral import datasets\n",
    "from spektral.layers import CensNetConv\n",
    "\n",
    "# Load the citation data.\n",
    "dataset = datasets.citation.Citation(\"cora\", normalize_x=True, random_splits=True)\n",
    "cora = dataset.read()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c82e46-ccf7-4c63-84c1-b3fdf9dfd33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert citation graph into an undirected graph.\n",
    "adjacency = cora.a.todense()\n",
    "adjacency_upper = tf.linalg.band_part(adjacency, 0, -1)\n",
    "adjacency_lower = tf.linalg.band_part(adjacency, -1, 0)\n",
    "\n",
    "adjacency_upper_symmetric = adjacency_upper + tf.transpose(adjacency_upper)\n",
    "adjacency_lower_symmetric = adjacency_lower + tf.transpose(adjacency_lower)\n",
    "adjacency_undirected = tf.maximum(adjacency_upper_symmetric, adjacency_lower_symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da84277e-a31f-4ce6-8e89-24bb62f032ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cotton_flower_mot.pipelines.model_training.graph_utils import compute_pairwise_similarities\n",
    "from cotton_flower_mot.pipelines.model_training.similarity_utils import cosine_similarity\n",
    "from spektral.data import Graph\n",
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "# Compute edge features.\n",
    "connected_node_indices = tf.where(adjacency_upper)\n",
    "# Get the corresponding node features for each edge.\n",
    "left_node_features = tf.gather(cora.x, connected_node_indices[:, 0])\n",
    "right_node_features = tf.gather(cora.x, connected_node_indices[:, 1])\n",
    "# Compute cosine similarities for each edge.\n",
    "cosine_similarities = cosine_similarity(left_node_features, right_node_features)\n",
    "\n",
    "edge_features = tf.expand_dims(cosine_similarities, -1)\n",
    "cora = Graph(x=cora.x, a=cora.a, e=edge_features.numpy(), y=cora.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d908ac0-865a-4c39-9612-12d1df82d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, layers\n",
    "from spektral.layers import CensNetConv\n",
    "\n",
    "# Build the model.\n",
    "node_features = Input(shape=(cora.n_node_features,))\n",
    "edge_features = Input(shape=(cora.n_edge_features,))\n",
    "node_laplacian = Input(shape=(cora.n_nodes,))\n",
    "# The undirected graph means that the number of edges is doubled.\n",
    "edge_laplacian = Input(shape=(cora.n_edges // 2,))\n",
    "incidence = Input(shape=(cora.n_edges // 2,))\n",
    "\n",
    "static_features = (node_laplacian, edge_laplacian, incidence)\n",
    "\n",
    "nodes_2, edges_2 = CensNetConv(64, 64, activation=\"relu\")((node_features, static_features, edge_features))\n",
    "nodes_2 = layers.Dropout(0.5)(nodes_2)\n",
    "edges_2 = layers.Dropout(0.5)(edges_2)\n",
    "nodes_3, _ = CensNetConv(64, 64, activation=\"relu\")((nodes_2, static_features, edges_2))\n",
    "nodes_3 = layers.Dropout(0.5)(nodes_3)\n",
    "# Apply the classification.\n",
    "node_class = layers.Dense(cora.n_labels, activation=\"softmax\")(nodes_3)\n",
    "\n",
    "model = Model(inputs=[node_features, edge_features, node_laplacian, edge_laplacian, incidence], outputs=[node_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a6f7aa-6954-4e87-84df-389595705c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data.loaders import SingleLoader\n",
    "import numpy as np\n",
    "\n",
    "# Prepare for training.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(reduction=\"sum\"),\n",
    "              weighted_metrics=[\"acc\"])\n",
    "\n",
    "# We convert the binary masks to sample weights so that we can compute the\n",
    "# average loss over the nodes (following original implementation by\n",
    "# Kipf & Welling)\n",
    "def mask_to_weights(mask):\n",
    "    return mask.astype(np.float32) / np.count_nonzero(mask)\n",
    "\n",
    "\n",
    "weights_tr, weights_va, weights_te = (\n",
    "    mask_to_weights(mask)\n",
    "    for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te)\n",
    ")\n",
    "\n",
    "node_laplacian, edge_laplacian, incidence = CensNetConv.preprocess(cora.a.todense())\n",
    "\n",
    "inputs_and_targets = ((cora.x, cora.e, node_laplacian, edge_laplacian, incidence), cora.y)\n",
    "training_dataset = tf.data.Dataset.from_tensors(inputs_and_targets + (weights_tr,))\n",
    "testing_dataset = tf.data.Dataset.from_tensors(inputs_and_targets + (weights_te,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce1b537f-7810-4078-8493-a25b774189d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "2022-06-20 14:16:02,296 - tensorflow - WARNING - Gradients do not exist for variables ['cens_net_conv_7/edge_kernel:0', 'cens_net_conv_7/node_weights:0', 'cens_net_conv_7/node_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "2022-06-20 14:16:02,453 - tensorflow - WARNING - Gradients do not exist for variables ['cens_net_conv_7/edge_kernel:0', 'cens_net_conv_7/node_weights:0', 'cens_net_conv_7/node_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.9459 - acc: 0.1429 - val_loss: 1.9414 - val_acc: 0.3190\n",
      "Epoch 2/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9462 - acc: 0.1500 - val_loss: 1.9435 - val_acc: 0.3190\n",
      "Epoch 3/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9452 - acc: 0.1286 - val_loss: 1.9460 - val_acc: 0.0910\n",
      "Epoch 4/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9483 - acc: 0.0857 - val_loss: 1.9473 - val_acc: 0.1030\n",
      "Epoch 5/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9465 - acc: 0.1357 - val_loss: 1.9479 - val_acc: 0.1300\n",
      "Epoch 6/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9459 - acc: 0.1071 - val_loss: 1.9478 - val_acc: 0.1300\n",
      "Epoch 7/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9454 - acc: 0.1500 - val_loss: 1.9471 - val_acc: 0.1300\n",
      "Epoch 8/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9466 - acc: 0.1000 - val_loss: 1.9462 - val_acc: 0.1300\n",
      "Epoch 9/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9482 - acc: 0.1071 - val_loss: 1.9453 - val_acc: 0.1380\n",
      "Epoch 10/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9499 - acc: 0.0857 - val_loss: 1.9448 - val_acc: 0.1440\n",
      "Epoch 11/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9456 - acc: 0.1786 - val_loss: 1.9439 - val_acc: 0.1470\n",
      "Epoch 12/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9427 - acc: 0.1643 - val_loss: 1.9435 - val_acc: 0.3120\n",
      "Epoch 13/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9481 - acc: 0.1500 - val_loss: 1.9435 - val_acc: 0.3190\n",
      "Epoch 14/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9492 - acc: 0.1357 - val_loss: 1.9443 - val_acc: 0.3190\n",
      "Epoch 15/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9461 - acc: 0.1571 - val_loss: 1.9450 - val_acc: 0.3130\n",
      "Epoch 16/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9501 - acc: 0.1143 - val_loss: 1.9457 - val_acc: 0.1050\n",
      "Epoch 17/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9476 - acc: 0.1500 - val_loss: 1.9465 - val_acc: 0.1490\n",
      "Epoch 18/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9464 - acc: 0.1429 - val_loss: 1.9469 - val_acc: 0.1490\n",
      "Epoch 19/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9463 - acc: 0.1357 - val_loss: 1.9473 - val_acc: 0.1500\n",
      "Epoch 20/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9433 - acc: 0.1357 - val_loss: 1.9470 - val_acc: 0.1400\n",
      "Epoch 21/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9476 - acc: 0.1286 - val_loss: 1.9466 - val_acc: 0.1300\n",
      "Epoch 22/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9476 - acc: 0.1214 - val_loss: 1.9457 - val_acc: 0.1310\n",
      "Epoch 23/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9391 - acc: 0.2143 - val_loss: 1.9445 - val_acc: 0.2060\n",
      "Epoch 24/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9458 - acc: 0.1643 - val_loss: 1.9441 - val_acc: 0.2610\n",
      "Epoch 25/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9424 - acc: 0.1643 - val_loss: 1.9437 - val_acc: 0.2730\n",
      "Epoch 26/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9478 - acc: 0.1143 - val_loss: 1.9441 - val_acc: 0.2600\n",
      "Epoch 27/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9454 - acc: 0.1286 - val_loss: 1.9445 - val_acc: 0.2260\n",
      "Epoch 28/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9437 - acc: 0.1071 - val_loss: 1.9456 - val_acc: 0.1120\n",
      "Epoch 29/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9452 - acc: 0.1286 - val_loss: 1.9467 - val_acc: 0.0990\n",
      "Epoch 30/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9415 - acc: 0.2000 - val_loss: 1.9473 - val_acc: 0.0970\n",
      "Epoch 31/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9450 - acc: 0.1357 - val_loss: 1.9472 - val_acc: 0.1060\n",
      "Epoch 32/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9434 - acc: 0.1643 - val_loss: 1.9456 - val_acc: 0.1340\n",
      "Epoch 33/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9373 - acc: 0.1857 - val_loss: 1.9425 - val_acc: 0.1820\n",
      "Epoch 34/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9383 - acc: 0.2000 - val_loss: 1.9395 - val_acc: 0.3230\n",
      "Epoch 35/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9372 - acc: 0.2143 - val_loss: 1.9370 - val_acc: 0.3590\n",
      "Epoch 36/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9373 - acc: 0.1857 - val_loss: 1.9355 - val_acc: 0.3570\n",
      "Epoch 37/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9344 - acc: 0.1857 - val_loss: 1.9327 - val_acc: 0.3650\n",
      "Epoch 38/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9214 - acc: 0.2786 - val_loss: 1.9283 - val_acc: 0.3530\n",
      "Epoch 39/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9147 - acc: 0.2571 - val_loss: 1.9225 - val_acc: 0.3550\n",
      "Epoch 40/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9100 - acc: 0.2786 - val_loss: 1.9148 - val_acc: 0.3580\n",
      "Epoch 41/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9060 - acc: 0.2429 - val_loss: 1.9060 - val_acc: 0.3620\n",
      "Epoch 42/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8902 - acc: 0.2929 - val_loss: 1.8954 - val_acc: 0.3730\n",
      "Epoch 43/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8777 - acc: 0.2857 - val_loss: 1.8840 - val_acc: 0.3910\n",
      "Epoch 44/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8790 - acc: 0.2857 - val_loss: 1.8751 - val_acc: 0.4320\n",
      "Epoch 45/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8432 - acc: 0.3429 - val_loss: 1.8686 - val_acc: 0.4070\n",
      "Epoch 46/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8403 - acc: 0.3000 - val_loss: 1.8586 - val_acc: 0.3920\n",
      "Epoch 47/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7986 - acc: 0.3357 - val_loss: 1.8391 - val_acc: 0.4540\n",
      "Epoch 48/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7668 - acc: 0.4071 - val_loss: 1.8162 - val_acc: 0.5040\n",
      "Epoch 49/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7567 - acc: 0.3857 - val_loss: 1.7862 - val_acc: 0.5290\n",
      "Epoch 50/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6946 - acc: 0.4357 - val_loss: 1.7464 - val_acc: 0.5500\n",
      "Epoch 51/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6299 - acc: 0.4643 - val_loss: 1.6986 - val_acc: 0.5510\n",
      "Epoch 52/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6344 - acc: 0.4786 - val_loss: 1.6505 - val_acc: 0.5850\n",
      "Epoch 53/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.5521 - acc: 0.4714 - val_loss: 1.6172 - val_acc: 0.6110\n",
      "Epoch 54/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4766 - acc: 0.5357 - val_loss: 1.5898 - val_acc: 0.5990\n",
      "Epoch 55/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4155 - acc: 0.5071 - val_loss: 1.5447 - val_acc: 0.5890\n",
      "Epoch 56/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3397 - acc: 0.5643 - val_loss: 1.4809 - val_acc: 0.6220\n",
      "Epoch 57/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3384 - acc: 0.5429 - val_loss: 1.4112 - val_acc: 0.6420\n",
      "Epoch 58/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2490 - acc: 0.5929 - val_loss: 1.3346 - val_acc: 0.6630\n",
      "Epoch 59/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1168 - acc: 0.6643 - val_loss: 1.2746 - val_acc: 0.6670\n",
      "Epoch 60/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1294 - acc: 0.6143 - val_loss: 1.2534 - val_acc: 0.6550\n",
      "Epoch 61/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1018 - acc: 0.6714 - val_loss: 1.2326 - val_acc: 0.6490\n",
      "Epoch 62/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0119 - acc: 0.6643 - val_loss: 1.1992 - val_acc: 0.6610\n",
      "Epoch 63/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0322 - acc: 0.6214 - val_loss: 1.1529 - val_acc: 0.6610\n",
      "Epoch 64/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8776 - acc: 0.7214 - val_loss: 1.1029 - val_acc: 0.6780\n",
      "Epoch 65/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8291 - acc: 0.7214 - val_loss: 1.0602 - val_acc: 0.6970\n",
      "Epoch 66/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8083 - acc: 0.7429 - val_loss: 1.0361 - val_acc: 0.7100\n",
      "Epoch 67/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7819 - acc: 0.7786 - val_loss: 1.0645 - val_acc: 0.7040\n",
      "Epoch 68/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7400 - acc: 0.7214 - val_loss: 1.1097 - val_acc: 0.6780\n",
      "Epoch 69/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6220 - acc: 0.8000 - val_loss: 1.1507 - val_acc: 0.6630\n",
      "Epoch 70/70\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6239 - acc: 0.8071 - val_loss: 1.1305 - val_acc: 0.6730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f274c216610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(training_dataset, validation_data=testing_dataset, epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcce235-662b-428c-8943-23a137453730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (cotton_flower_mot)",
   "language": "python",
   "name": "kedro_cotton_flower_mot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
