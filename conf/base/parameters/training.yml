# Tensorboard parameters.
# Log directory.
tensorboard_output_dir: ${log_dir}/tensorboard
# Period at which to write histograms, in epochs.
histogram_period: 2
# Period at which to write metrics, in batches.
update_period: 100
# Size of generated heatmap visualizations. (width, height).
heatmap_size: [960, 540]
# Period in epochs at which to visualize heatmaps.
heatmap_period: 5
# Number of batches to visualize heatmaps from.
num_heatmap_batches: 2
# Number of heatmaps to visualize in each batch.
num_heatmap_images: 3

# Whether to enable debugging checks for NaN and infinity. This
# is very useful when trying to debug numerical issues, but it
# slows down training a lot.
enable_numeric_checks: false
# Whether to enable mixed precision training, which can save memory and run
# faster on newer hardware.
enable_mixed_precision: true

loss_params:
  # Alpha constant for heatmap focal loss.
  alpha: 2.0
  # Beta constant for heatmap focal loss.
  beta: 4.0
  # Weight to use for size regression loss.
  size_weight: 1.0
  # Weight to use for offset regression loss.
  offset_weight: 1.0

# Weights for each loss term.
heatmap_loss_weight: 1.0
geometry_loss_weight: 0.1
sinkhorn_loss_weight: 10.0

# Fraction of pre-trained layers to keep frozen when performing
# transfer learning.
freeze_fraction: 0.5

# Learning phases to use for training the model.
learning_phases:
  # The initial learning rate to use for the phase.
  - learning_rate:
      # Initial learning rate value.
      initial: 0.001
      # Specify that we want to decay over time.
      decay: false
      # Number of steps to decay over.
      # (422 steps per epoch)
      decay_steps: 5064
      # Used to derive the number of iterations in the i-th period.
      t_mul: 2.0
      # Used to derive the initial learning rate of the i-th period.
      m_mul: 1.0
      # How many epochs of no improvement we tolerate before reducing the LR,
      # if we are not using SGDR.
      lr_patience_epochs: 2
      # Minimum learning rate to hit.
      min_learning_rate: 0.000001
      # Number of LR warmup steps. Setting this to zero disables warmup.
      warmup_steps: 0
    # The momentum to use for the phase.
    momentum: 0.9
    # Number of epochs to train for in this phase.
    num_epochs: 45

# Learning phases for training RotNet.
rotnet_learning_phases:
  # The initial learning rate to use for the phase.
  - learning_rate:
      # Initial learning rate value.
      initial: 0.001
      # Specify that we want to decay over time.
      decay: false
      # Number of steps to decay over.
      # (440 steps per epoch)
      decay_steps: 1320
      # Used to derive the number of iterations in the i-th period.
      t_mul: 2.0
      # Used to derive the initial learning rate of the i-th period.
      m_mul: 1.0
      # How many epochs of no improvement we tolerate before reducing the LR,
      # if we are not using SGDR.
      lr_patience_epochs: 2
      # Minimum learning rate to hit.
      min_learning_rate: 0.00001
      # Number of LR warmup steps. Setting this to zero disables warmup.
      warmup_steps: 0
    # The momentum to use for the phase.
    momentum: 0.9
    # Number of epochs to train for in this phase.
    num_epochs: 1


# Learning phases for training colorization.
colorization_learning_phases:
  # The initial learning rate to use for the phase.
  - learning_rate:
      # Initial learning rate value.
      initial: 0.001
      # Specify that we want to decay over time.
      decay: false
      # Number of steps to decay over.
      # (440 steps per epoch)
      decay_steps: 1320
      # Used to derive the number of iterations in the i-th period.
      t_mul: 2.0
      # Used to derive the initial learning rate of the i-th period.
      m_mul: 1.0
      # How many epochs of no improvement we tolerate before reducing the LR,
      # if we are not using SGDR.
      lr_patience_epochs: 2
      # Minimum learning rate to hit.
      min_learning_rate: 0.00001
      # Number of LR warmup steps. Setting this to zero disables warmup.
      warmup_steps: 0
    # The momentum to use for the phase.
    momentum: 0.9
    # Number of epochs to train for in this phase.
    num_epochs: 1
